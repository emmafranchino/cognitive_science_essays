@article{Caramazza_2000,
   abstract = {There are two views about the nature of consonants and vowels. One view holds that they are categorically distinct objects that play a fundamental role in the construction of syllables in speech production1,2,3. The other view is that they are convenient labels for distinguishing between peak (vowel) and non-peak (consonant) parts of a continuous stream of sound that varies in sonority (roughly the degree of openness of the vocal apparatus during speech)4,5,6, or that they are summary labels for bundles of feature segments7,8. Taking the latter view, consonants and vowels do not have an independent status in language processing. Here we provide evidence for the possible categorical distinction between consonants and vowels in the brain. We report the performance of two Italian-speaking aphasics who show contrasting, selective difficulties in producing vowels and consonants. Their performance in producing individual consonants is independent of the sonority value and feature properties of the consonants. This pattern of results suggests that consonants and vowels are processed by distinct neural mechanisms, thereby providing evidence for their independent status in language production.},
   author = {Alfonso Caramazza and Doriana Chialant and Rita Capasso and Gabriele Miceli},
   doi = {10.1038/35000206},
   issn = {1476-4687},
   issue = {6768},
   journal = {Nature},
   pages = {428-430},
   title = {Separable processing of consonants and vowels},
   volume = {403},
   url = {https://doi.org/10.1038/35000206},
   year = {2000},
}
@article{Carreiras_2008,
   abstract = {Previous behavioral and electrophysiological studies have shown dissociation between consonants and vowels. We used functional magnetic resonance imaging to investigate whether vowel and consonant processing differences are expressed in the neuronal activation pattern and whether they are modulated by task. The experimental design involved reading aloud and lexical decision on visually presented pseudowords created by transposing or replacing consonants or vowels in words. During reading aloud, changing vowels relative to consonants increased activation in a right middle temporal area previously associated with prosodic processing of speech input. In contrast, during lexical decision, changing consonants relative to vowels increased activation in a right middle frontal area associated with inhibiting go-responses. The task-sensitive nature of these effects demonstrates that consonants and vowels differ at a processing, rather than stimulus, level. We argue that prosodic processing of vowel changes arise during self-monitoring of speech output, whereas greater inhibition of go-responses to consonant changes follows insufficient lexico-semantic processing when nonwords looking particularly like words must be rejected. Our results are consistent with claims that vowels and consonants place differential demands on prosodic and lexico-semantic processing, respectively. They also highlight the different types of information that can be drawn from functional imaging and neuropsychological studies.},
   author = {Manuel Carreiras and Cathy J Price},
   doi = {10.1093/cercor/bhm202},
   issn = {1047-3211},
   issue = {7},
   journal = {Cerebral Cortex},
   month = {7},
   pages = {1727-1735},
   title = {Brain Activation for Consonants and Vowels},
   volume = {18},
   url = {https://doi.org/10.1093/cercor/bhm202},
   year = {2008},
}
@article{Boatman_1997,
   abstract = {The effects of direct cortical electrical interference on consonant and vowel discrimination were investigated in five patients with implanted subdural electrode arrays. Without electrical interference, patients' performance discriminating consonants and vowels was intact. With electrical interference, consonant discrimination was impaired at one electrode site in each patient on the superior temporal gyrus of the lateral left perisylvian cortex. Conversely, vowel and tone discrimination remained relatively intact when tested with electrical interference at the same site. Analysis of patients' consonant discrimination errors revealed that neither differences in acoustic temporal structure nor syllable position fully account for the consonantvowel perceptual dissociations elicited. Our data suggest that at the cortical level consonant and vowel perception are intrinsically distinct perceptual phenomena. The selective impairment of consonant, but not vowel, discrimination further suggests that consonant and vowel perception are distinguished by differences in relative dependence on the functional - perhaps integrative - resources of the left lateral superior temporal gyrus.},
   author = {D Boatman and C Hall and M H Goldstein and R Lesser and B Gordon},
   doi = {https://doi.org/10.1016/S0010-9452(97)80006-8},
   issn = {0010-9452},
   issue = {1},
   journal = {Cortex},
   pages = {83-98},
   title = {Neuroperceptual Differences in Consonant and Vowel Discrimination: As Revealed by Direct Cortical Electrical Interference},
   volume = {33},
   url = {https://www.sciencedirect.com/science/article/pii/S0010945297800068},
   year = {1997},
}
@article{Toro_2023,
   abstract = {Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language. Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing. More concretely, just like humans, ChatGPT has a consonant bias. That is, the chatbot has a tendency to use consonants over vowels to identify words. This is observed across languages that differ in their relative distribution of consonants and vowels such as English and Spanish. Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT.},
   author = {Juan Manuel Toro},
   doi = {https://doi.org/10.48550/arXiv.2305.15929},
   keywords = {Artificial intelligence,ChatGPT,Consonant bias,Word recognition},
   title = {Emergence of a phonological bias in ChatGPT},
   year = {2023},
}
@article{Bonatti_2005,
   abstract = { Speech is produced mainly in continuous streams containing several words. Listeners can use the transitional probability (TP) between adjacent and non-adjacent syllables to segment “words” from a continuous stream of artificial speech, much as they use TPs to organize a variety of perceptual continua. It is thus possible that a general-purpose statistical device exploits any speech unit to achieve segmentation of speech streams. Alternatively, language may limit what representations are open to statistical investigation according to their specific linguistic role. In this article, we focus on vowels and consonants in continuous speech. We hypothesized that vowels and consonants in words carry different kinds of information, the latter being more tied to word identification and the former to grammar. We thus predicted that in a word identification task involving continuous speech, learners would track TPs among consonants, but not among vowels. Our results show a preferential role for consonants in word identification. },
   author = {Luca L Bonatti and Marcela Peña and Marina Nespor and Jacques Mehler},
   doi = {10.1111/j.0956-7976.2005.01556.x},
   issue = {6},
   journal = {Psychological Science},
   note = {PMID: 15943671},
   pages = {451-459},
   title = {Linguistic Constraints on Statistical Computations: The Role of Consonants and Vowels in Continuous Speech Processing},
   volume = {16},
   url = {https://doi.org/10.1111/j.0956-7976.2005.01556.x},
   year = {2005},
}
@article{Ooijen_1996,
   abstract = {Laboratoire de Sciences Cognitives et Psycholinguistique, Paris, France This study introduces a new paradigm for investigating lexical processing. First, an analysis of data from a series of word-spotting experiments is presented suggesting that listeners treat vowels as more mutable than consonants in auditory word recognition in English. In order to assess this hypothesis, a word reconstruction task was devised in which listeners were required to turn word-like nonwords into words by adapting the identity of either one vowel or one consonant. Listeners modified vowel identity more readily than consonant identity. Furthermore, incorrect responses more often involved a vowel change than a consonant change. These findings are compatible with the proposal that English listeners are equipped to deal with vowel variability by assuming that vowel identity is comparatively underdefined. The results are discussed in the light of theoretical accounts of speech processing.},
   author = {Brit Van Ooijen},
   doi = {10.3758/BF03201084},
   issn = {1532-5946},
   issue = {5},
   journal = {Memory & Cognition},
   pages = {573-583},
   title = {Vowel mutability and lexical selection in English: Evidence from a word reconstruction task},
   volume = {24},
   url = {https://doi.org/10.3758/BF03201084},
   year = {1996},
}
@article{Toro_2008,
   abstract = { We have proposed that consonants give cues primarily about the lexicon, whereas vowels carry cues about syntax. In a study supporting this hypothesis, we showed that when segmenting words from an artificial continuous stream, participants compute statistical relations over consonants, but not over vowels. In the study reported here, we tested the symmetrical hypothesis that when participants listen to words in a speech stream, they tend to exploit relations among vowels to extract generalizations, but tend to disregard the same relations among consonants. In our streams, participants could segment words on the basis of transitional probabilities in one tier and could extract a structural regularity in the other tier. Participants used consonants to extract words, but vowels to extract a structural generalization. They were unable to extract the same generalization using consonants, even when word segmentation was facilitated and the generalization made simpler. Our results suggest that different signal-driven computations prime lexical and grammatical processing. },
   author = {Juan M Toro and Marina Nespor and Jacques Mehler and Luca L Bonatti},
   doi = {10.1111/j.1467-9280.2008.02059.x},
   issue = {2},
   journal = {Psychological Science},
   note = {PMID: 18271861},
   pages = {137-144},
   title = {Finding Words and Rules in a Speech Stream: Functional Differences Between Vowels and Consonants},
   volume = {19},
   url = {https://doi.org/10.1111/j.1467-9280.2008.02059.x},
   year = {2008},
}
@article{Parea_2004,
   abstract = {Nonwords created by transposing two adjacent letters (i.e., transposed-letter (TL) nonwords like jugde) are very effective at activating the lexical representation of their base words. This fact poses problems for most computational models of word recognition (e.g., the interactive-activation model and its extensions), which assume that exact letter positions are rapidly coded during the word recognition process. To examine the scope of TL similarity effects further, we asked whether TL similarity effects occur for nonwords created by exchanging two nonadjacent letters (e.g., caniso-CASINO) in three masked form priming experiments using the lexical decision task. The two nonadjacent transposed letters were consonants in Experiment 1 (e.g., caniso-CASINO), vowels in Experiment 2 (anamil-ANIMAL) and both consonants and vowels in Experiment 3. Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants. In a final experiment we examined latencies for nonwords created by nonadjacent transpositions of consonants versus vowels in a lexical decision task. Both types of nonwords produced longer latencies than matched controls, with consonant TL nonwords being more difficult than vowel TL nonwords. The implications of these findings for models having “position-specific” coding schemes as well as for models proposing alternative coding schemes are discussed.},
   author = {Manuel Perea and Stephen J Lupker},
   doi = {https://doi.org/10.1016/j.jml.2004.05.005},
   issn = {0749-596X},
   issue = {2},
   journal = {Journal of Memory and Language},
   keywords = {Coding schemes,Lexical decision,Orthographic similarity,Transposed letters,Vowel/consonant processing},
   pages = {231-246},
   title = {Can CANISO activate CASINO? Transposed-letter similarity effects with nonadjacent letter positions},
   volume = {51},
   url = {https://www.sciencedirect.com/science/article/pii/S0749596X04000592},
   year = {2004},
}
@article{Nazzi_2015,
author = {Bouchon, Camillia and Floccia, Caroline and Fux, Thibaut and Adda-Decker, Martine and Nazzi, Thierry},
title = {Call me Alix, not Elix: vowels are more important than consonants in own-name recognition at 5 months},
journal = {Developmental Science},
volume = {18},
number = {4},
pages = {587-598},
doi = {https://doi.org/10.1111/desc.12242},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/desc.12242},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/desc.12242},
abstract = {Abstract Consonants and vowels differ acoustically and articulatorily, but also functionally: Consonants are more relevant for lexical processing, and vowels for prosodic/syntactic processing. These functional biases could be powerful bootstrapping mechanisms for learning language, but their developmental origin remains unclear. The relative importance of consonants and vowels at the onset of lexical acquisition was assessed in French-learning 5-month-olds by testing sensitivity to minimal phonetic changes in their own name. Infants’ reactions to mispronunciations revealed sensitivity to vowel but not consonant changes. Vowels were also more salient (on duration and intensity) but less distinct (on spectrally based measures) than consonants. Lastly, vowel (but not consonant) mispronunciation detection was modulated by acoustic factors, in particular spectrally based distance. These results establish that consonant changes do not affect lexical recognition at 5 months, while vowel changes do; the consonant bias observed later in development does not emerge until after 5 months through additional language exposure.},
year = {2015}
}
@article{Toro_2019,
   abstract = {In natural languages, vowels tend to convey structures (syntax, prosody) while consonants are more important lexically. The consonant bias, which is the tendency to rely more on consonants than on vowels to process words, is well attested in human adults and infants after the first year of life. Is the consonant bias based on evolutionarily ancient mechanisms, potentially present in other species? The current study investigated this issue in a species phylogenetically distant from humans: Long–Evans rats. During training, the animals were presented with four natural word-forms (e.g., mano, “hand”). We then compared their responses to novel words carrying either a consonant (pano) or a vowel change (meno). Results show that the animals were less disrupted by consonantal alterations than by vocalic alterations of words. That is, word recognition was more affected by the alteration of a vowel than a consonant. Together with previous findings in very young human infants, this reliance on vocalic information we observe in rats suggests that the emergence of the consonant bias may require a combination of vocal, cognitive and auditory skills that rodents do not seem to possess.},
   author = {Camillia Bouchon and Juan M Toro},
   doi = {10.1007/s10071-019-01280-3},
   issn = {1435-9456},
   issue = {5},
   journal = {Animal Cognition},
   pages = {839-850},
   title = {Is the consonant bias specifically human? Long–Evans rats encode vowels better than consonants in words},
   volume = {22},
   url = {https://doi.org/10.1007/s10071-019-01280-3},
   year = {2019},
}
@article{Mallikarjun_2021,
   abstract = {Consonants and vowels play different roles in speech perception: listeners rely more heavily on consonant information rather than vowel information when distinguishing between words. This reliance on consonants for word identification is the consonant bias Nespor et al. (Ling 2:203–230, 2003). Several factors modulate infants’ development of the consonant bias, including fine-grained temporal processing ability and native language exposure [for review, see Nazzi et al.  (Curr Direct Psychol Sci 25:291–296, 2016)]. A rat model demonstrated that mature fine-grained temporal processing alone cannot account for consonant bias emergence; linguistic exposure is also necessary Bouchon and Toro (An Cog 22:839–850, 2019). This study tested domestic dogs, who have similarly fine-grained temporal processing but more language exposure than rats, to assess whether a minimal lexicon and small degree of regular linguistic exposure can allow for consonant bias development. Dogs demonstrated a vowel bias rather than a consonant bias, preferring their own name over a vowel-mispronounced version of their name, but not in comparison to a consonant-mispronounced version. This is the pattern seen in young infants Bouchon et al. (Dev Sci 18:587–598, 2015) and rats Bouchon et al.  (An Cog 22:839–850, 2019). In a follow-up study, dogs treated a consonant-mispronounced version of their name similarly to their actual name, further suggesting that dogs do not treat consonant differences as meaningful for word identity. These results support the findings from Bouchon and Toro (An Cog 2:839–850, 2019), suggesting that there may be a default preference for vowel information over consonant information when identifying word forms, and that the consonant bias may be a human-exclusive tool for language learning.},
   author = {Amritha Mallikarjun and Emily Shroads and Rochelle S Newman},
   doi = {10.1007/s10071-020-01436-6},
   issn = {1435-9456},
   issue = {3},
   journal = {Animal Cognition},
   pages = {419-431},
   title = {The role of linguistic experience in the development of the consonant bias},
   volume = {24},
   url = {https://doi.org/10.1007/s10071-020-01436-6},
   year = {2021},
}


